---
title: "WWW 2018 Challenge: Learning to Recognize Musical Genre"
layout: posts
author_profile: true
---

I won the "WWW 2018 Challenge: Learning to Recognize Musical Genre" with [Jay Kim](https://www.tudelft.nl/en/eemcs/the-faculty/departments/intelligent-systems/multimedia-computing/people/jaehun-jay-kim/). This post describes our approach for the challenge. Full paper is available [here](https://dl.acm.org/citation.cfm?id=3191823).

![alt text](../images/www/leader.png "Final result")
*Final result*


## Intro
### About challenge
This year, the Web conference, also known as WWW, newly organized a challenge track. [Learning to Recognize Musical Genre](https://www.crowdai.org/challenges/www-2018-challenge-learning-to-recognize-musical-genre) was one of four programs in the challenge track. A goal of the challenge was to recognize the musical genre of a piece of music of which only a recording is available. The data was a subset of [FMA dataset](https://github.com/mdeff/fma).

### Evaluation
The challenge consists of two rounds. In the first round, participants are provided a test set of 35,000 clips of 30 seconds each, and they have to submit their predictions for all the 35,000 clips. The primary metric for evaluation was the Mean Log Loss. In the second round, which is the final round, participants have to wrap their models in a Docker container. Organizers evaluate those against a new unseen test set.

### Team
I teamed up with [Jay Kim](https://www.tudelft.nl/en/eemcs/the-faculty/departments/intelligent-systems/multimedia-computing/people/jaehun-jay-kim/), who is my previous colleague at [MARG](http://marg.snu.ac.kr/) and currently a PhD student at TU Delft.

## Our approach
### Motivation
In the beginning of the challenge, each of us had own idea for the model design. So, we decided to work separately and ensemble the learned features from each model. However, during the experiment, we figured out some critical factors that can affect the final result.

- Since the FMA genre annotations have been done by uploaders, they are noisy and not reliable. (I think someone has to report errors and refine the dataset.)
- Most of genre annotations have been done in album level even if they have multiple genres in a single album.
- There are a lot of duplicated artists in a train set.
- For round 1, test accuracy does not follow the validation accuracy.

Participants could check their results on the leader board interactively for the round 1. We got the best result when we overfit our model to the train set and stop the iteration at a certain heuristic point. One of our speculation for this phenomenon was that the test set of round 1 has shared artists with the train set.

Since we didn't know the artist distribution of the round 2 and we wanted our model to learn more generalized representations, we needed more reliable targets. To this end, Jay proposed to use clusters as targets of our models instead of noisy genre annotations.

### Proposed method
#### Artist Group Factors (AGFs)

#### Model design