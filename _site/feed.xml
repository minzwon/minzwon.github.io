<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-05-13T01:53:25+02:00</updated><id>http://localhost:4000/</id><title type="html">Minz Won</title><subtitle>Exploring music semantics with machines.</subtitle><author><name>-</name><email>minz.won@upf.edu</email></author><entry><title type="html">WWW 2018 Challenge: Learning to Recognize Musical Genre</title><link href="http://localhost:4000/WWW/" rel="alternate" type="text/html" title="WWW 2018 Challenge: Learning to Recognize Musical Genre" /><published>2018-05-11T00:00:00+02:00</published><updated>2018-05-11T00:00:00+02:00</updated><id>http://localhost:4000/WWW</id><content type="html" xml:base="http://localhost:4000/WWW/">&lt;p&gt;I won the “WWW 2018 Challenge: Learning to Recognize Musical Genre” with &lt;a href=&quot;https://www.tudelft.nl/en/eemcs/the-faculty/departments/intelligent-systems/multimedia-computing/people/jaehun-jay-kim/&quot;&gt;Jay Kim&lt;/a&gt;. This post describes our approach for the challenge. Full paper is available &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3191823&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/www/leader.png&quot; alt=&quot;alt text&quot; title=&quot;Final result&quot; /&gt;
&lt;em&gt;Final result&lt;/em&gt; (&lt;a href=&quot;https://zenodo.org/record/1243501#.WvVzUdOFNge&quot;&gt;link&lt;/a&gt;)&lt;/p&gt;

&lt;h1 id=&quot;intro&quot;&gt;Intro&lt;/h1&gt;
&lt;h2 id=&quot;about-challenge&quot;&gt;About challenge&lt;/h2&gt;
&lt;p&gt;This year, the Web conference, also known as WWW, newly organized a challenge track. &lt;a href=&quot;https://www.crowdai.org/challenges/www-2018-challenge-learning-to-recognize-musical-genre&quot;&gt;Learning to Recognize Musical Genre&lt;/a&gt; was one of four programs in the challenge track. A goal of our challenge was to recognize the musical genre of a piece of music of which only a recording is available. The data was a subset of &lt;a href=&quot;https://github.com/mdeff/fma&quot;&gt;FMA dataset&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;The challenge consists of two rounds. In the first round, participants are provided a test set of 35,000 clips of 30 seconds each, and they have to submit their predictions for all the 35,000 clips. The primary metric for evaluation was the Mean Log Loss. In the second round, which is the final round, participants have to wrap their models in a Docker container. Organizers evaluate those against a new unseen test set.&lt;/p&gt;

&lt;h2 id=&quot;team&quot;&gt;Team&lt;/h2&gt;
&lt;p&gt;I teamed up with &lt;a href=&quot;https://www.tudelft.nl/en/eemcs/the-faculty/departments/intelligent-systems/multimedia-computing/people/jaehun-jay-kim/&quot;&gt;Jay Kim&lt;/a&gt;, who is my previous colleague at &lt;a href=&quot;http://marg.snu.ac.kr/&quot;&gt;MARG&lt;/a&gt; and currently a PhD student at TU Delft.&lt;/p&gt;

&lt;h1 id=&quot;our-approach&quot;&gt;Our approach&lt;/h1&gt;
&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;In the beginning of the challenge, each of us had own idea for the model design. So, we decided to work separately and ensemble the learned features from each model. However, during the experiment, we figured out some critical factors that can affect the final result.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Since the FMA genre annotations have been done by uploaders, they are noisy and not reliable. (I think someone has to report errors and refine the dataset.)&lt;/li&gt;
  &lt;li&gt;Most of genre annotations (5028 out of 5152 albums) have been done in album level even if they have multiple genres in a single album.&lt;/li&gt;
  &lt;li&gt;There are a lot of duplicated artists in a train set. (25000 tracks of train set were from 5152 unique albums.)&lt;/li&gt;
  &lt;li&gt;For round 1, test accuracy does not follow the validation accuracy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Participants could check their results on the leader board interactively for the round 1. We got the best result when we overfit our model to the train set and stop the iteration at a certain heuristic point. One of our speculation for this phenomenon was that the test set of round 1 has shared artists with the train set.&lt;/p&gt;

&lt;p&gt;Since we didn’t know the artist distribution of the round 2 and we wanted our model to learn more generalized representations, we needed more reliable targets to learn such representations. To this end, Jay proposed to &lt;strong&gt;use clusters as targets&lt;/strong&gt; of our models instead of noisy genre annotations.&lt;/p&gt;

&lt;h2 id=&quot;proposed-method&quot;&gt;Proposed method&lt;/h2&gt;
&lt;h3 id=&quot;artist-group-factors-agfs&quot;&gt;Artist Group Factors (AGFs)&lt;/h3&gt;
&lt;p&gt;Due to the reasons that I mentioned above, targetting artist label might be beneficial for this challenge. There was also a &lt;a href=&quot;https://arxiv.org/abs/1710.06648&quot;&gt;previous research&lt;/a&gt; that utilized artist labels for the representation learning. However, due to data sparsity, only a few tracks are assigned per artist. It can be beneficial to group artist labels into clusters of similar artists, avoiding learning bottlenecks caused by large numbers of classes. To this end, we proposed Artist Group Factors (AGFs).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/www/agf.png&quot; alt=&quot;alt text&quot; title=&quot;AGFs&quot; /&gt;
&lt;em&gt;AGF extraction pipeline&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The main idea of extracting AGFs is to cluster artists based on meaningful feature sets that allow for aggregation at (and beyond) the artist level. Let’s take a look at the pipeline step-by-step.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;From the music data, extract features or tags. i.e. &lt;a href=&quot;http://essentia.upf.edu/documentation/streaming_extractor_music.html&quot;&gt;Essentia features&lt;/a&gt;, Mel-Frequency Cepstral Coefficients (MFCC), dMFCC, or subgenre tags.&lt;/li&gt;
  &lt;li&gt;Learn feature dictionaries with obtained features using K-Means clustering. If you use tags, no need for this step.&lt;/li&gt;
  &lt;li&gt;Each artist can be represented using a Bag-of-Word (BoW) feature vector. For example, let’s assume that we have 5 clusters from previous step, and there is an artist called Barbara. If three songs of Barbara belong to cluster #1 and four songs belong to cluster #3, her feature vector might be [3, 0, 4, 0, 0]. We can also do the same with tags.&lt;/li&gt;
  &lt;li&gt;Apply Latent Dirichlet Allocation (LDA) to transform artist-level BoW vectors into AGF representations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We generated four different AGFs using Essentia features, MFCC, dMFCC, and subgenre tags.&lt;/p&gt;

&lt;h3 id=&quot;model-design&quot;&gt;Model design&lt;/h3&gt;
&lt;p&gt;We trained five networks targetting genre tags and each of four AGFs. Same Convolutional Neural Network (CNN) structures have been used. You can check detailed structure from the &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=3191823&quot;&gt;paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/www/transfer.png&quot; alt=&quot;alt text&quot; title=&quot;transfer&quot; /&gt;
&lt;em&gt;Transfer learning scenario&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Finally, we transferred all of learned representations to predict genre tags which was the goal of this challenge. A simple Multi Layer Perceptron (MLP) has been used for this step.&lt;/p&gt;

&lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;FMA dataset has to be refined.&lt;/li&gt;
  &lt;li&gt;Log-loss of the round 2 was higher than our expectation even though we used AGFs. But we don’t know the distribution of round 2 test set.&lt;/li&gt;
  &lt;li&gt;To determine the effectiveness of AGFs, they have to be testified on other dataset with more elaborate split methods.&lt;/li&gt;
  &lt;li&gt;Evaluation metrics such as LogLoss, F1 are imperfect. Higher score in these metrics do not always mean the better model.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Next destination… &lt;a href=&quot;https://recsys-challenge.spotify.com/&quot;&gt;Spotify-RecSys Challenge&lt;/a&gt;&lt;/p&gt;</content><author><name>-</name><email>minz.won@upf.edu</email></author><summary type="html">I won the “WWW 2018 Challenge: Learning to Recognize Musical Genre” with Jay Kim. This post describes our approach for the challenge. Full paper is available here.</summary></entry><entry><title type="html">dl4mir docker image</title><link href="http://localhost:4000/dl4mir/" rel="alternate" type="text/html" title="dl4mir docker image" /><published>2018-05-10T00:00:00+02:00</published><updated>2018-05-10T00:00:00+02:00</updated><id>http://localhost:4000/dl4mir</id><content type="html" xml:base="http://localhost:4000/dl4mir/">&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dl4mir&lt;/code&gt; is a docker image for the deep learning for music information retrieval research. (&lt;a href=&quot;https://github.com/minzwon/docker_dl4mir&quot;&gt;Github&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For music analysis: &lt;code class=&quot;highlighter-rouge&quot;&gt;essentia&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;librosa&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;madmom&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For deep learning: &lt;code class=&quot;highlighter-rouge&quot;&gt;pytorch&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;keras&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;sklearn&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;cuda&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;and some useful python libraries including: &lt;code class=&quot;highlighter-rouge&quot;&gt;numpy&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;scipy&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;pandas&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;matplotlib&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;seaborn&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;jupyter&lt;/code&gt; and more!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;versions&quot;&gt;Versions&lt;/h2&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dl4mir:gpu-py2&lt;/code&gt; python2 environment&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dl4mir:gpu-py3&lt;/code&gt; python3 environment&lt;/p&gt;

&lt;h2 id=&quot;guideline&quot;&gt;Guideline&lt;/h2&gt;

&lt;h3 id=&quot;pull-docker-image&quot;&gt;Pull docker image&lt;/h3&gt;
&lt;p&gt;Just type:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker pull minzwon/dl4mir:gpu-py2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker pull minzwon/dl4mir:gpu-py3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;run-docker-container&quot;&gt;Run docker container&lt;/h3&gt;
&lt;p&gt;You should run the image with &lt;code class=&quot;highlighter-rouge&quot;&gt;nvidia-docker&lt;/code&gt; to use your GPUs.&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nvidia-docker run -d -it -p 8888:8888 -v HOST_DIRECTORY:CONTAINER_DIRECTORY --name ANYNAME minzwon/dl4mir:gpu-py3 /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;-d&lt;/code&gt; is a detached mode. This makes your container to run on the background. You can continue the container with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nvidia-docker exec -it ANYNAME /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>-</name><email>minz.won@upf.edu</email></author><summary type="html">dl4mir is a docker image for the deep learning for music information retrieval research. (Github)</summary></entry><entry><title type="html">Benvinguda</title><link href="http://localhost:4000/benvinguda/" rel="alternate" type="text/html" title="Benvinguda" /><published>2018-05-01T00:00:00+02:00</published><updated>2018-05-01T00:00:00+02:00</updated><id>http://localhost:4000/benvinguda</id><content type="html" xml:base="http://localhost:4000/benvinguda/">&lt;h4 id=&quot;just-started-sunny&quot;&gt;Just started :sunny:&lt;/h4&gt;</content><author><name>-</name><email>minz.won@upf.edu</email></author><summary type="html">Just started :sunny:</summary></entry></feed>